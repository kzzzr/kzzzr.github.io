---

---
Models prefixed with **_stg_ _**comprise the so-called [_staging area_](https://github.com/kzzzr/mybi-dbt-core/tree/master/models/staging). Source tables are green-coloured, staging tables are blue on the picture below (click to expand):

[![](https://habrastorage.org/webt/ty/4y/eg/ty4yeg_ijmb3nemejt0cunglxh0.gif)](https://habrastorage.org/webt/ty/4y/eg/ty4yeg_ijmb3nemejt0cunglxh0.gif)

Since I want to use dbt to deploy my projects I would like to build a common ground. In other words that would be staging area pointing to source tables covered with data tests thoroughly.

Let us examine the source code for [stg_ym_sessions_facts](https://github.com/kzzzr/mybi-dbt-core/blob/master/models/staging/metrika/stg_ym_sessions_facts.sql):

{% highlight sql %}
{% raw %}

with source as (
 
select
 
     {{ surrogate_key(["account_id",
       "clientids_id",
       "dates_id",
       "traffic_id",
       "locations_id",
       "devices_id"])
       }} as id
   , f.account_id
   , f.clientids_id
   , f.dates_id
   , f.traffic_id
   , f.locations_id
   , f.devices_id
   , f.sessions
   , f.bounces
   , f.pageviews
   , f.duration
   , gd.dt
   , gd.ts
 
from {{ source('metrika', 'sessions_facts') }} as f
   left join {{ ref('stg_general_dates') }} as gd
       on gd.id = f.dates_id
 
{{ filter_rows(
   account_id=var('account_id_metrika'),
   last_number_of_days=true,
   ts_field='dt'
) }}
 
)
 
select * from source

{% endraw %}
{% endhighlight %}

Staging tables serve the following key purposes:

1. Listing available columns

So that any subsequent model could use all of them or specific columns.

2. Simple joins are performed

See the **_'stg_general_dates_'** is joined to enrich the staging table with basic time dimension: session date and timestamp. You might also want to add weekday, quarter, date of the week start, quarter, etc.

3. Surrogate keys

Are generated where applicable. Macro is used which I will describe in detail later.

4. Rows are filtered

Whole logic is implemented in **_filter_rows_** macro. Key point here is you may have several source accounts streaming data to a single table, each of them having its own unique identifier. I only want to fetch rows for a specific account (customer).

There is also one major improvement for the Continuous Integration pipeline: using only 3 last days of data to test your DWH before you merge a new patch. I will cover it later.

Other things you can do in the staging area:

* Rename your columns to align different sources
* Cast field types, cast timezones (local -> utc)
* Deduplicate rows if your ELT service doesnâ€™t do so